[General]
output = /path/to/output
trainingVariables = variable0,variable1,variable2
samples = Sample1,Sample2
lumi = 1.0
testPercentage = 0.2
selection = None
ShuffleData = True
SuffleSeed = None

[Sample1]
input = /path/to/input.h5
label = Sample1Label
xsec = 0.25
nGen = 100000
datatype = mc

[Sample2]
input = /path/to/input.h5
label = Sample2Label
xsec = -1
nGen = -1
datatype = data
selection = SomeSel

[NeuralNet]
activation = relu
outputActivation = softmax
useWeightDecay = False
weightDecayLambda = 0.001
name = AllOptDNN
layerDimentions = 20,20
optimizer = adagrad
inputDimention = 10
epochs = 10
validationSplit = 0.3
loss = categorical_crossentropy
batchSize = 128
doEarlyStopping = True
patience = 25

